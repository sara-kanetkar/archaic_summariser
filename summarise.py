# -*- coding: utf-8 -*-
"""summarise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vxyPv-V38vxyhX0TeVHkHZXo2kYCfVov
"""



import json
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
import torch
from transformers import Seq2SeqTrainingArguments
from transformers import TrainingArguments

import sys
!{sys.executable} -m pip install torch

from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

!pip install colab-xterm

# Commented out IPython magic to ensure Python compatibility.
# %load_ext colabxterm

text = """
Friends, Romans, countrymen, lend me your ears;
I come to bury Caesar, not to praise him.
The evil that men do lives after them;
The good is oft interred with their bones;
So let it be with Caesar. The noble Brutus
Hath told you Caesar was ambitious:
If it were so, it was a grievous fault,
And grievously hath Caesar answer‚Äôd it.
Here, under leave of Brutus and the rest‚Äì
For Brutus is an honourable man;
So are they all, all honourable men‚Äì
Come I to speak in Caesar‚Äôs funeral.
He was my friend, faithful and just to me:
But Brutus says he was ambitious;
And Brutus is an honourable man.
He hath brought many captives home to Rome
Whose ransoms did the general coffers fill:
Did this in Caesar seem ambitious?
When that the poor have cried, Caesar hath wept:
Ambition should be made of sterner stuff:
Yet Brutus says he was ambitious;
And Brutus is an honourable man.
You all did see that on the Lupercal
I thrice presented him a kingly crown,
Which he did thrice refuse: was this ambition?
Yet Brutus says he was ambitious;
And, sure, he is an honourable man.
I speak not to disprove what Brutus spoke,
But here I am to speak what I do know.
You all did love him once, not without cause:
What cause withholds you then, to mourn for him?
O judgment! thou art fled to brutish beasts,
And men have lost their reason. Bear with me;
My heart is in the coffin there with Caesar,
And I must pause till it come back to me.
"""

summary = summarizer(text, max_length=50, min_length=25, do_sample=False)

print("Summary:\n", summary[0]['summary_text'])

pip install transformers gradio torch

from transformers import pipeline
import gradio as gr

classifier = pipeline("zero-shot-classification")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
possible_topics = ["sports", "politics", "medicine", "technology", "finance"]

def summarize_by_topic(text):
    classification = classifier(text, possible_topics)
    topic = classification['labels'][0]

    summary = summarizer(text, max_length=100, min_length=25, do_sample=False)[0]['summary_text']

    return f"üß† **Detected Topic:** {topic}\n\nüìù **Summary:** {summary}"

!pip install pymupdf
!pip install python-pptx

import fitz  # PyMuPDF
from pptx import Presentation
from transformers import pipeline
from io import BytesIO

# Initialize summarizer pipeline (you can specify a smaller model if needed)
summarizer = pipeline("summarization")

def summarize_file(file):
    try:
        file_ext = file.name.split(".")[-1].lower()
        with open(file.name, "rb") as f:
            file_bytes = f.read()

        text = ""
        if file_ext == "pdf":
            doc = fitz.open(stream=file_bytes, filetype="pdf")
            for page in doc:
                text += page.get_text()
        elif file_ext == "pptx":
            with open("temp.pptx", "wb") as f:
                f.write(file_bytes)
            prs = Presentation("temp.pptx")
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"
        elif file_ext == "txt":
            text = file_bytes.decode("utf-8")
        else:
            return "Unsupported file format."

        if not text.strip():
            return "No readable text found in the uploaded file."

        # Debug: show some text
        print("Extracted text preview:\n", text[:300])

        # Summarization in chunks
        max_chunk_size = 1000
        chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        summaries = [summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']
                     for chunk in chunks]

        return "\n\n".join(summaries)

    except Exception as e:
        return f"Error occurred: {str(e)}"

!pip install gradio

from google.colab import files
uploaded = files.upload()

#fine tuning
from google.colab import files
uploaded = files.upload()

import os
print(os.listdir('/content'))

# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])

!pip install transformers datasets accelerate

pip install transformers datasets torch sklearn

# Split dataset (80% train, 20% validation)
dataset = dataset.train_test_split(test_size=0.2, seed=42)

pip install transformers==4.28.1  # or another stable version

import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"translate archaic to modern: {text}" for text in examples["source"]]

    targets = examples["target"]

    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    labels["input_ids"] = [
    [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
    for labels_seq in labels["input_ids"]
]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=20,


    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")

from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: knowledge is power")
print(result[0]['summary_text'])

from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",  # Your retrained model
    tokenizer="./shakespeare_summarizer_final"
)

# Test mixed examples
texts = [
    "All that glitters is not gold.",  # Archaic
    "My friend wrote this while working at Google.",  # Modern

]

for text in texts:
    summary = summarizer(text)  # No prefix needed if you removed it during training
    print(f"Original: {text}")
    print(f"Summary: {summary[0]['summary_text']}\n")

!jupyter nbconvert --to script your_notebook_name.ipynb

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import pipeline
# 
# # Load your fine-tuned model
# summarizer = pipeline("summarization", model="shakespeare_summarizer_final", tokenizer="shakespeare_summarizer_final")
# 
# st.title("Archaic Text Summarizer üìù")
# st.subheader("Convert old English to modern summaries")
# 
# input_text = st.text_area("Enter archaic text here:")
# 
# if st.button("Summarize"):
#     if input_text.strip():
#         summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)
#         st.success("Summary:")
#         st.write(summary[0]['summary_text'])
#     else:
#         st.warning("Please enter some text.")
#

!ls

# Commented out IPython magic to ensure Python compatibility.
# %%writefile requirements.txt
# streamlit
# transformers
# torch
# 
#

!git clone https://github.com/sara-kanetkar/archaic_summariser.git

!mv app.py archaic_summariser/
!mv requirements.txt archaic_summariser/
!mv shakespeare_summarizer_final archaic_summariser/

!git config --global user.email "sarakanetkar@gmail.com"
!git config --global user.name "Sara Kanetkar"

from google.colab import files
files.download('app.py')

!zip -r model.zip shakespeare_summarizer_final

from google.colab import files
files.download('model.zip')

# Commented out IPython magic to ensure Python compatibility.

# %cd archaic_summariser
!git add .
!git commit -m "Added Streamlit app and fine-tuned model"
!git push origin main

!pip install streamlit
import streamlit as st
from transformers import pipeline

st.title("Archaic Summarizer")

text_input = st.text_area("Enter archaic English text here:")
if st.button("Summarize"):
    summarizer = pipeline("summarization", model="path/to/your/fine-tuned-model")
    summary = summarizer(text_input, max_length=50, min_length=25, do_sample=False)
    st.subheader("Summary:")
    st.write(summary[0]['summary_text'])

!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook "summarise.ipynb"

# Commented out IPython magic to ensure Python compatibility.
# %cd content

!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook summarise.ipynb

!pwd
!ls

import IPython

# Save the notebook as an .ipynb file
IPython.get_ipython().run_line_magic('save', 'summarise 0-999')

# Rename the file to summarise.ipynb if needed
!mv summarise.py summarise.ipynb

!ls

from google.colab import files
files.download("summarise.ipynb")

# -------- Gradio Interface -------- #
file_tab = gr.Interface(
    fn=summarize_file,
    inputs=gr.File(file_types=[".pdf", ".pptx", ".txt"], label="Upload File"),
    outputs="text",
    title="üìÇ File Summarizer"
)

text_tab = gr.Interface(
    fn=summarize_by_topic,
    inputs=gr.Textbox(lines=10, label="Paste Lecture or Article Text"),
    outputs="text",
    title="üß† Topic Classifier + Summarizer"
)

# Combine both in Tabs
gr.TabbedInterface(
    interface_list=[file_tab, text_tab],
    tab_names=["üìÅ Summarize File", "üìã Classify & Summarize Text"]
).launch()