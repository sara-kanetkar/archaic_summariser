# coding: utf-8

get_ipython().system('jupyter nbconvert --to script your_notebook_name.ipynb')
get_ipython().run_cell_magic('writefile', 'app.py', 'import streamlit as st\nfrom transformers import pipeline\n\n# Load your fine-tuned model\nsummarizer = pipeline("summarization", model="shakespeare_summarizer_final", tokenizer="shakespeare_summarizer_final")\n\nst.title("Archaic Text Summarizer üìù")\nst.subheader("Convert old English to modern summaries")\n\ninput_text = st.text_area("Enter archaic text here:")\n\nif st.button("Summarize"):\n    if input_text.strip():\n        summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)\n        st.success("Summary:")\n        st.write(summary[0][\'summary_text\'])\n    else:\n        st.warning("Please enter some text.")\n')
get_ipython().system('ls')
from transformers import pipeline

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
text = """
Friends, Romans, countrymen, lend me your ears;
I come to bury Caesar, not to praise him.
The evil that men do lives after them;
The good is oft interred with their bones;
So let it be with Caesar. The noble Brutus
Hath told you Caesar was ambitious:
If it were so, it was a grievous fault,
And grievously hath Caesar answer‚Äôd it.
Here, under leave of Brutus and the rest‚Äì
For Brutus is an honourable man;
So are they all, all honourable men‚Äì
Come I to speak in Caesar‚Äôs funeral.
He was my friend, faithful and just to me:
But Brutus says he was ambitious;
And Brutus is an honourable man.
He hath brought many captives home to Rome
Whose ransoms did the general coffers fill:
Did this in Caesar seem ambitious?
When that the poor have cried, Caesar hath wept:
Ambition should be made of sterner stuff:
Yet Brutus says he was ambitious;
And Brutus is an honourable man.
You all did see that on the Lupercal
I thrice presented him a kingly crown,
Which he did thrice refuse: was this ambition?
Yet Brutus says he was ambitious;
And, sure, he is an honourable man.
I speak not to disprove what Brutus spoke,
But here I am to speak what I do know.
You all did love him once, not without cause:
What cause withholds you then, to mourn for him?
O judgment! thou art fled to brutish beasts,
And men have lost their reason. Bear with me;
My heart is in the coffin there with Caesar,
And I must pause till it come back to me.
"""
summary = summarizer(text, max_length=50, min_length=25, do_sample=False)
print("Summary:\n", summary[0]['summary_text'])
classifier = pipeline("zero-shot-classification")
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
possible_topics = ["sports", "politics", "medicine", "technology", "finance"]
def summarize_by_topic(text):
    classification = classifier(text, possible_topics)
    topic = classification['labels'][0]

    summary = summarizer(text, max_length=100, min_length=25, do_sample=False)[0]['summary_text']

    return f"üß† **Detected Topic:** {topic}\n\nüìù **Summary:** {summary}"
import fitz  # PyMuPDF
from pptx import Presentation
from transformers import pipeline
from io import BytesIO

# Initialize summarizer pipeline (you can specify a smaller model if needed)
summarizer = pipeline("summarization")

def summarize_file(file):
    try:
        file_ext = file.name.split(".")[-1].lower()
        with open(file.name, "rb") as f:
            file_bytes = f.read()

        text = ""
        if file_ext == "pdf":
            doc = fitz.open(stream=file_bytes, filetype="pdf")
            for page in doc:
                text += page.get_text()
        elif file_ext == "pptx":
            with open("temp.pptx", "wb") as f:
                f.write(file_bytes)
            prs = Presentation("temp.pptx")
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"
        elif file_ext == "txt":
            text = file_bytes.decode("utf-8")
        else:
            return "Unsupported file format."

        if not text.strip():
            return "No readable text found in the uploaded file."

        # Debug: show some text
        print("Extracted text preview:\n", text[:300])

        # Summarization in chunks
        max_chunk_size = 1000
        chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        summaries = [summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']
                     for chunk in chunks]

        return "\n\n".join(summaries)

    except Exception as e:
        return f"Error occurred: {str(e)}"
get_ipython().system('pip install pymupdf')
get_ipython().system('pip install python-pptx')
import fitz  # PyMuPDF
from pptx import Presentation
from transformers import pipeline
from io import BytesIO

# Initialize summarizer pipeline (you can specify a smaller model if needed)
summarizer = pipeline("summarization")

def summarize_file(file):
    try:
        file_ext = file.name.split(".")[-1].lower()
        with open(file.name, "rb") as f:
            file_bytes = f.read()

        text = ""
        if file_ext == "pdf":
            doc = fitz.open(stream=file_bytes, filetype="pdf")
            for page in doc:
                text += page.get_text()
        elif file_ext == "pptx":
            with open("temp.pptx", "wb") as f:
                f.write(file_bytes)
            prs = Presentation("temp.pptx")
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        text += shape.text + "\n"
        elif file_ext == "txt":
            text = file_bytes.decode("utf-8")
        else:
            return "Unsupported file format."

        if not text.strip():
            return "No readable text found in the uploaded file."

        # Debug: show some text
        print("Extracted text preview:\n", text[:300])

        # Summarization in chunks
        max_chunk_size = 1000
        chunks = [text[i:i + max_chunk_size] for i in range(0, len(text), max_chunk_size)]
        summaries = [summarizer(chunk, max_length=150, min_length=30, do_sample=False)[0]['summary_text']
                     for chunk in chunks]

        return "\n\n".join(summaries)

    except Exception as e:
        return f"Error occurred: {str(e)}"
from google.colab import files
uploaded = files.upload()
import os
print(os.listdir('/content'))
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"s {text}" for text in examples["source"]]
    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])
get_ipython().system('pip install gradio')
get_ipython().run_line_magic('pip', 'install transformers gradio torch')
# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])
# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])
# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])
# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])
# Re-upload dataset if needed
from google.colab import files
uploaded = files.upload()

# Reload dataset
import json
from datasets import Dataset
with open('/content/shakespeare_dataset.json') as f:
    data = json.load(f)
dataset = Dataset.from_list(data)


# Check the first example
print(dataset[0])
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"s {text}" for text in examples["source"]]
    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: Every cloud has a silver lining.")
print(result[0]['summary_text'])
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"s {text}" for text in examples["source"]]
    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: Every cloud has a silver lining.")
print(result[0]['summary_text'])
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"translate archaic to modern: {text}" for text in examples["source"]]

    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    labels["input_ids"] = [
    [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
    for labels_seq in labels["input_ids"]
]
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=3,
    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"translate archaic to modern: {text}" for text in examples["source"]]

    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    labels["input_ids"] = [
    [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
    for labels_seq in labels["input_ids"]
]
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=10,
save_strategy="epoch"

    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"translate archaic to modern: {text}" for text in examples["source"]]

    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    labels["input_ids"] = [
    [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
    for labels_seq in labels["input_ids"]
]
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=10,
    save_strategy="epoch"

    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"translate archaic to modern: {text}" for text in examples["source"]]

    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    labels["input_ids"] = [
    [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
    for labels_seq in labels["input_ids"]
]
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=10,
    

    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: Every cloud has a silver lining.")
print(result[0]['summary_text'])
import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import Dataset
import torch
from sklearn.model_selection import train_test_split

# Load and prepare the dataset
with open('shakespeare_dataset.json', 'r') as f:
    data = json.load(f)

# Split into train and validation sets
train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_dict({
    'source': [item['source'] for item in train_data],
    'target': [item['target'] for item in train_data]
})

val_dataset = Dataset.from_dict({
    'source': [item['source'] for item in val_data],
    'target': [item['target'] for item in val_data]
})

# Load tokenizer and model
model_name = "t5-small"  # or "facebook/bart-base" for BART
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Tokenization function
def preprocess_function(examples):
    inputs = [f"translate archaic to modern: {text}" for text in examples["source"]]

    targets = examples["target"]
    
    model_inputs = tokenizer(
        inputs, max_length=128, truncation=True, padding="max_length"
    )
    
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            targets, max_length=64, truncation=True, padding="max_length"
        )
    labels["input_ids"] = [
    [(label if label != tokenizer.pad_token_id else -100) for label in labels_seq]
    for labels_seq in labels["input_ids"]
]
    
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply tokenization
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_val = val_dataset.map(preprocess_function, batched=True)

# Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="./shakespeare_summarizer",
    eval_strategy="epoch",  # Changed from evaluation_strategy to eval_strategy
    learning_rate=3e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=20,
    

    predict_with_generate=True,
    fp16=torch.cuda.is_available(),
    report_to="none",
)


# Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
)

# Start training
trainer.train()

# Save the fine-tuned model
model.save_pretrained("./shakespeare_summarizer_final")
tokenizer.save_pretrained("./shakespeare_summarizer_final")
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: Every cloud has a silver lining.")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: knowledge is power")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: actions speak louder than words")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: time heals all wounds")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: better late tan never")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: better late than never")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",
    tokenizer="./shakespeare_summarizer_final"
)

result = summarizer("summarize archaic text: knowledge is power")
print(result[0]['summary_text'])
from transformers import pipeline

summarizer = pipeline(
    "summarization",
    model="./shakespeare_summarizer_final",  # Your retrained model
    tokenizer="./shakespeare_summarizer_final"
)

# Test mixed examples
texts = [
    "All that glitters is not gold.",  # Archaic
    "My friend wrote this while working at Google.",  # Modern
    
]

for text in texts:
    summary = summarizer(text)  # No prefix needed if you removed it during training
    print(f"Original: {text}")
    print(f"Summary: {summary[0]['summary_text']}\n")
get_ipython().system('jupyter nbconvert --to script your_notebook_name.ipynb')
get_ipython().run_cell_magic('writefile', 'app.py', 'import streamlit as st\nfrom transformers import pipeline\n\n# Load your fine-tuned model\nsummarizer = pipeline("summarization", model="shakespeare_summarizer_final", tokenizer="shakespeare_summarizer_final")\n\nst.title("Archaic Text Summarizer üìù")\nst.subheader("Convert old English to modern summaries")\n\ninput_text = st.text_area("Enter archaic text here:")\n\nif st.button("Summarize"):\n    if input_text.strip():\n        summary = summarizer(input_text, max_length=100, min_length=30, do_sample=False)\n        st.success("Summary:")\n        st.write(summary[0][\'summary_text\'])\n    else:\n        st.warning("Please enter some text.")\n')
get_ipython().system('ls')
get_ipython().system('git clone https://github.com/your-username/your-repo-name.git')
get_ipython().system('git clone https://github.com/your-username/sarakanetkar.git')
get_ipython().system('git clone https://github.com/your-username/sara-kanetkar.git')
get_ipython().run_cell_magic('writefile', 'requirements.txt', 'streamlit\ntransformers\ntorch\n\n')
get_ipython().system('git config --global user.name "sara kanetkar"')
import streamlit as st
from transformers import pipeline

st.title("Archaic Summarizer")

text_input = st.text_area("Enter archaic English text here:")
if st.button("Summarize"):
    summarizer = pipeline("summarization", model="path/to/your/fine-tuned-model")
    summary = summarizer(text_input, max_length=50, min_length=25, do_sample=False)
    st.subheader("Summary:")
    st.write(summary[0]['summary_text'])
get_ipython().system('pip install streamlit')
import streamlit as st
from transformers import pipeline

st.title("Archaic Summarizer")

text_input = st.text_area("Enter archaic English text here:")
if st.button("Summarize"):
    summarizer = pipeline("summarization", model="path/to/your/fine-tuned-model")
    summary = summarizer(text_input, max_length=50, min_length=25, do_sample=False)
    st.subheader("Summary:")
    st.write(summary[0]['summary_text'])
get_ipython().system('git config --global user.name "sara-kanetkar"')
get_ipython().system('git config --global user.name "sara-kanetkar"')
get_ipython().system('git clone https://github.com/sara-kanetkar/archaic_summariser.git')
get_ipython().system('git clone https://github.com/sara-kanetkar/archaic_summariser.git')
get_ipython().system('mv app.py archaic_summariser/')
get_ipython().system('mv requirements.txt archaic_summariser/')
get_ipython().system('mv shakespeare_summarizer_final archaic_summariser/')
get_ipython().system('git config --global user.email "sarakanetkar@gmail.com"')
get_ipython().system('git config --global user.name "Sara Kanetkar"')
get_ipython().run_line_magic('cd', 'archaic_summariser')
get_ipython().system('git add .')
get_ipython().system('git commit -m "Added Streamlit app and fine-tuned model"')
get_ipython().system('git push origin main')
get_ipython().run_line_magic('cd', 'archaic_summariser')
get_ipython().system('git add .')
get_ipython().system('git commit -m "Added Streamlit app and fine-tuned model"')
get_ipython().system('git push origin main')
from google.colab import files
files.download('app.py')
get_ipython().system('zip -r model.zip shakespeare_summarizer_final')
from google.colab import files
files.download('model.zip')
get_ipython().system('jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook your_notebook_name.ipynb')
get_ipython().system('jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook LLM(4).ipynb')
get_ipython().system('jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook "LLM(4).ipynb"')
get_ipython().system('git add llm_cleaned.ipynb')
get_ipython().system('git commit -m "Cleaned notebook metadata"')
get_ipython().system('git push origin main')
from google.colab import files
files.download("LLM(4).ipynb")
get_ipython().system('jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook "summarise.ipynb"')
from google.colab import files
files.download("summarise.ipynb")
# Go back to the parent directory
get_ipython().run_line_magic('cd', '..')

# Download the notebook
from google.colab import files
files.download("summarise.ipynb")
get_ipython().run_line_magic('cd', 'content')
get_ipython().system('jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace --clear-output --to notebook summarise.ipynb')
from google.colab import files
files.download("summarise.ipynb")
get_ipython().system('pwd')
get_ipython().system('ls')
import IPython

# Save the notebook as an .ipynb file
IPython.get_ipython().run_line_magic('save', 'summarise 0-999')

# Rename the file to summarise.ipynb if needed
get_ipython().system('mv summarise.py summarise.ipynb')
